{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Decision Optimization model with Watson Machine Learning\n",
    "\n",
    "This notebook shows you how to deploy a Decision Optimization model, create and monitor jobs, and get solutions using the Watson Machine Learning Python Client.\n",
    "\n",
    "This notebook runs on Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Install the Watson Machine Learning client API](#setup)\n",
    "2. [Create a client instance](#create)\n",
    "3. [Prepare your model archive](#prepare)\n",
    "4. [Upload your model on Watson Machine Learning](#upload)\n",
    "5. [Create a deployment](#deploy)\n",
    "6. [Create and monitor a job with inline data for your deployed model](#job)\n",
    "7. [Display the solution](#display)\n",
    "8. [Solve another problem using the same deployment](#problem)\n",
    "9. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "### Set up the Watson Machine Learning client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and then import the Watson Machine Learning client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install WML client API\n",
    "\n",
    "!pip install ibm-watson-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create'></a>\n",
    "### Create a client instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a client using credentials\n",
    "\n",
    "cluster = \"<your_cluster>\"\n",
    "username = \"<username>\"\n",
    "password = \"<password>\"\n",
    "\n",
    "wml_credentials = {\n",
    "\"username\": username,\n",
    "\"password\": password,\n",
    "\"instance_id\" : \"wml_local\",\n",
    "\"url\": cluster,\n",
    "\"version\": \"4.6\"  \n",
    "}\n",
    "\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare'></a>\n",
    "### Prepare your model archive\n",
    "\n",
    "Put the model.py file in a subdirectory and create a tar.gz file. The model consists of two parts:\n",
    "* some functions that create an `inputs` dictionary from files and that create files from an `outputs` dictionary,\n",
    "* the optimization model that uses the inputs and outputs dictionaries.\n",
    "\n",
    "Use the `write_file` command to write these models to a `main.py` file. \n",
    "\n",
    "Use the `tar` command to create a tar archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/main.py\n",
    "\n",
    "from docplex.util.environment import get_environment\n",
    "import pandas\n",
    "from six import iteritems\n",
    "from collections.abc import Mapping\n",
    "from os.path import join, dirname, basename, splitext, exists\n",
    "import glob\n",
    "\n",
    "class _InputDict(dict):\n",
    "    def __init__(self, directory, names):\n",
    "        dict.__init__(self)\n",
    "        self._directory = directory\n",
    "        for k in names:\n",
    "            dict.__setitem__(self, k, None)\n",
    "        file='model_schema.json'\n",
    "        if self._directory is not None:\n",
    "            file  = \"{0}/\".format(self._directory) + file\n",
    "        self.dtype_schemas = self.get_dtype_schemas( file)\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            item = dict.__getitem__(self, key)\n",
    "            if item is None:\n",
    "                file = \"{0}.csv\".format(key)\n",
    "                if file in self.dtype_schemas:\n",
    "                    return self.read_df( key, dtype=self.dtype_schemas[file])\n",
    "                else:\n",
    "                    return self.read_df( key)\n",
    "            else:\n",
    "                return item\n",
    "        else:\n",
    "            raise Exception(\"Accessing input dict via non string index\")\n",
    "    def read_df(self, key, **kwargs):\n",
    "        env = get_environment()\n",
    "        file = \"{0}.csv\".format(key)\n",
    "        if self._directory is not None:\n",
    "            file  = \"{0}/\".format(self._directory) + file\n",
    "        with env.get_input_stream(file) as ist:\n",
    "            params = {'encoding': 'utf8'}\n",
    "            if kwargs:\n",
    "                params.update(kwargs)\n",
    "            df = pandas.read_csv( ist, **params)\n",
    "            dict.__setitem__(self, key, df)\n",
    "        return df\n",
    "    def get_dtype_schemas(self, path):\n",
    "        dtype_schemas = {}\n",
    "        if exists(path):\n",
    "            input_schemas=json.load(open(path))\n",
    "            if 'input' in input_schemas:\n",
    "                for input_schema in input_schemas['input']:\n",
    "                    dtype_schema = {}\n",
    "                    if 'fields' in input_schema:\n",
    "                        for input_schema_field in input_schema['fields']:\n",
    "                            if input_schema_field['type']=='string':\n",
    "                                dtype_schema[input_schema_field['name']]='str'\n",
    "                        if len(dtype_schema) > 0:\n",
    "                            dtype_schemas[input_schema['id']]=dtype_schema\n",
    "        print(dtype_schemas)\n",
    "        return dtype_schemas\n",
    "\n",
    "class _LazyDict(Mapping):\n",
    "    def __init__(self, *args, **kw):\n",
    "        self._raw_dict = _InputDict(*args, **kw)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._raw_dict.__getitem__(key)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._raw_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._raw_dict)\n",
    "\n",
    "    def read_df(self, key, **kwargs):\n",
    "        return self._raw_dict.read_df(key, **kwargs)\n",
    "\n",
    "def get_all_inputs(directory=None):\n",
    "    '''Utility method to read a list of files and return a tuple with all\n",
    "    read data frames.\n",
    "    Returns:\n",
    "        a map { datasetname: data frame }\n",
    "    '''\n",
    "\n",
    "    all_csv = \"*.csv\"\n",
    "    g = join(directory, all_csv) if directory else all_csv\n",
    "\n",
    "    names = [splitext(basename(f))[0] for f in glob.glob(g)]\n",
    "    result = _LazyDict(directory, names)\n",
    "    return result\n",
    "\n",
    "def write_all_outputs(outputs):\n",
    "    '''Write all dataframes in ``outputs`` as .csv.\n",
    "\n",
    "    Args:\n",
    "        outputs: The map of outputs 'outputname' -> 'output df'\n",
    "    '''\n",
    "    for (name, df) in iteritems(outputs):\n",
    "        csv_file = '%s.csv' % name\n",
    "        print(csv_file)\n",
    "        with get_environment().get_output_stream(csv_file) as fp:\n",
    "            if sys.version_info[0] < 3:\n",
    "                fp.write(df.to_csv(index=False, encoding='utf8'))\n",
    "            else:\n",
    "                fp.write(df.to_csv(index=False).encode(encoding='utf8'))\n",
    "    if len(outputs) == 0:\n",
    "        print(\"Warning: no outputs written\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a model/main.py\n",
    "\n",
    "# Load CSV files into inputs dictionnary\n",
    "inputs = get_all_inputs()\n",
    "\n",
    "food = inputs['diet_food']\n",
    "nutrients = inputs['diet_nutrients']\n",
    "food_nutrients = inputs['diet_food_nutrients']\n",
    "food_nutrients.set_index('Food', inplace=True)\n",
    "        \n",
    "from docplex.mp.model import Model\n",
    "\n",
    "# Model\n",
    "mdl = Model(name='diet')\n",
    "\n",
    "# Create decision variables, limited to be >= Food.qmin and <= Food.qmax\n",
    "qty = food[['name', 'qmin', 'qmax']].copy()\n",
    "qty['var'] = qty.apply(lambda x: mdl.continuous_var(lb=x['qmin'],\n",
    "                                                ub=x['qmax'],\n",
    "                                                name=x['name']),\n",
    "                   axis=1)\n",
    "# make the name the index\n",
    "qty.set_index('name', inplace=True)\n",
    "\n",
    "# Limit range of nutrients, and mark them as KPIs\n",
    "for n in nutrients.itertuples():\n",
    "    amount = mdl.sum(qty.loc[f.name]['var'] * food_nutrients.loc[f.name][n.name]\n",
    "                     for f in food.itertuples())\n",
    "    mdl.add_range(n.qmin, amount, n.qmax)\n",
    "    mdl.add_kpi(amount, publish_name='Total %s' % n.name)\n",
    "\n",
    "# Minimize cost\n",
    "obj = mdl.sum(qty.loc[f.name]['var'] * f.unit_cost for f in food.itertuples())\n",
    "mdl.add_kpi(obj, publish_name=\"Minimal cost\");\n",
    "mdl.minimize(obj)\n",
    "\n",
    "mdl.print_information()\n",
    "\n",
    "# solve\n",
    "ok = mdl.solve()\n",
    "\n",
    "mdl.print_solution()\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "solution_df = pandas.DataFrame(columns=['Food', 'value'])\n",
    "\n",
    "for index, dvar in enumerate(mdl.solution.iter_variables()):\n",
    "    solution_df.loc[index,'Food'] = dvar.to_string()\n",
    "    solution_df.loc[index,'value'] = dvar.solution_value\n",
    "    \n",
    "outputs = {}\n",
    "outputs['solution'] = solution_df\n",
    "        \n",
    "# Generate output files\n",
    "write_all_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "def reset(tarinfo):\n",
    "    tarinfo.uid = tarinfo.gid = 0\n",
    "    tarinfo.uname = tarinfo.gname = \"root\"\n",
    "    return tarinfo\n",
    "tar = tarfile.open(\"model.tar.gz\", \"w:gz\")\n",
    "tar.add(\"model/main.py\", arcname=\"main.py\", filter=reset)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='upload'></a>\n",
    "### Upload your model on Watson Machine Learning\n",
    "\n",
    "Store model in Watson Machine Learning with:\n",
    "* the tar archive previously created,\n",
    "* metadata including the model type and runtime\n",
    "\n",
    "Get the `model_uid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All available meta data properties \n",
    "\n",
    "client.repository.ModelMetaNames.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to list deployment spaces and delete any that are no longer needed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.spaces.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.spaces.delete(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.spaces.get_details(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the space ID\n",
    "\n",
    "space_name = \"<space_name>\"\n",
    "\n",
    "space_id = [x['metadata']['id'] for x in client.spaces.get_details()['resources'] if x['entity']['name'] == space_name][0]\n",
    "\n",
    "client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_metadata = {\n",
    "    client.repository.ModelMetaNames.NAME: \"Diet\",\n",
    "    client.repository.ModelMetaNames.DESCRIPTION: \"Model for Diet\",\n",
    "    client.repository.ModelMetaNames.TYPE: \"do-docplex_20.1\",\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: client.software_specifications.get_uid_by_name(\"do_20.1\"),\n",
    "    # OPTIONAL but can be interesting for a better inference of string data if needed\n",
    "    client.repository.ModelMetaNames.INPUT_DATA_SCHEMA: [ \n",
    "        { \"id\" : \"diet_food_nutrients.csv\",  \"fields\" : [ \n",
    "            { \"name\" : \"Food\", \"type\" : \"string\" }, { \"name\" : \"Calories\", \"type\" : \"double\" }, { \"name\" : \"Calcium\", \"type\" : \"double\" }, { \"name\" : \"Iron\", \"type\" : \"double\" }, \n",
    "            { \"name\" : \"Vit_A\", \"type\" : \"double\" }, { \"name\" : \"Dietary_Fiber\", \"type\" : \"double\" }, { \"name\" : \"Carbohydrates\", \"type\" : \"double\" }, { \"name\" : \"Protein\", \"type\" : \"double\" } ] }, \n",
    "        { \"id\" : \"diet_food.csv\", \"fields\" : [ \n",
    "            { \"name\" : \"name\", \"type\" : \"string\" }, { \"name\" : \"unit_cost\", \"type\" : \"double\" }, { \"name\" : \"qmin\", \"type\" : \"double\" }, { \"name\" : \"qmax\", \"type\" : \"double\" } ] }, \n",
    "        { \"id\" : \"diet_nutrients.csv\", \"fields\" : [ \n",
    "            { \"name\" : \"name\", \"type\" : \"string\" }, { \"name\" : \"qmin\", \"type\" : \"double\" }, { \"name\" : \"qmax\", \"type\" : \"double\" } ] } ],\n",
    "    # OPTIONAL but can be interesting for a better inference of string data mainly in context of inline data if needed\n",
    "    client.repository.ModelMetaNames.OUTPUT_DATA_SCHEMA: [ \n",
    "        { \"id\" : \"stats.csv\", \"fields\" : [ \n",
    "            { \"name\" : \"Name\", \"type\" : \"string\" }, { \"name\" : \"Value\", \"type\" : \"string\" } ] }, \n",
    "        { \"id\" : \"solution.csv\", \"fields\" : [ \n",
    "            { \"name\" : \"name\", \"type\" : \"string\" }, { \"name\" : \"value\", \"type\" : \"double\" } ] }, \n",
    "        { \"id\" : \"kpis.csv\", \"fields\" : [ \n",
    "            { \"name\" : \"Name\", \"type\" : \"string\" }, { \"name\" : \"Value\", \"type\" : \"double\" } ] } ]\n",
    "}\n",
    "\n",
    "model_details = client.repository.store_model(model='/home/wsuser/work/model.tar.gz', meta_props=mnist_metadata)\n",
    "\n",
    "model_uid = client.repository.get_model_id(model_details)\n",
    "\n",
    "# print model uid if needed\n",
    "# print( model_uid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deploy'></a>\n",
    "### Create a deployment \n",
    "\n",
    "Create a batch deployment for the model, providing information such as:\n",
    "* the maximum number of compute nodes\n",
    "* the T-shirt size of the compute nodes\n",
    "\n",
    "Get the `deployment_uid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_props = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"Diet Deployment\",\n",
    "    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"Diet Deployment\",\n",
    "    client.deployments.ConfigurationMetaNames.BATCH: {},\n",
    "    client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: {'name': 'S', 'num_nodes': 1}\n",
    "}\n",
    "\n",
    "deployment_details = client.deployments.create(model_uid, meta_props=meta_props)\n",
    "\n",
    "deployment_uid = client.deployments.get_uid(deployment_details)\n",
    "\n",
    "# print deployment id if needed\n",
    "# print( deployment_uid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# List all existing deployments\n",
    "\n",
    "client.deployments.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='job'></a>\n",
    "### Create and monitor a job with inline data for your deployed model\n",
    "\n",
    "Create a payload containing inline input data.\n",
    "\n",
    "Create a new job with this payload and the deployment.\n",
    "\n",
    "Get the `job_uid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library \n",
    "import pandas as pd \n",
    "  \n",
    "# initialize list of lists \n",
    "diet_food = pd.DataFrame([ [\"Roasted Chicken\", 0.84, 0, 10],\n",
    "                [\"Spaghetti W/ Sauce\", 0.78, 0, 10],\n",
    "                [\"Tomato,Red,Ripe,Raw\", 0.27, 0, 10],\n",
    "                [\"Apple,Raw,W/Skin\", 0.24, 0, 10],\n",
    "                [\"Grapes\", 0.32, 0, 10],\n",
    "                [\"Chocolate Chip Cookies\", 0.03, 0, 10],\n",
    "                [\"Lowfat Milk\", 0.23, 0, 10],\n",
    "                [\"Raisin Brn\", 0.34, 0, 10],\n",
    "                [\"Hotdog\", 0.31, 0, 10]] , columns = [\"name\",\"unit_cost\",\"qmin\",\"qmax\"])\n",
    "\n",
    "diet_food_nutrients = pd.DataFrame([\n",
    "                [\"Spaghetti W/ Sauce\", 358.2, 80.2, 2.3, 3055.2, 11.6, 58.3, 8.2],\n",
    "                [\"Roasted Chicken\", 277.4, 21.9, 1.8, 77.4, 0, 0, 42.2],\n",
    "                [\"Tomato,Red,Ripe,Raw\", 25.8, 6.2, 0.6, 766.3, 1.4, 5.7, 1],\n",
    "                [\"Apple,Raw,W/Skin\", 81.4, 9.7, 0.2, 73.1, 3.7, 21, 0.3],\n",
    "                [\"Grapes\", 15.1, 3.4, 0.1, 24, 0.2, 4.1, 0.2],\n",
    "                [\"Chocolate Chip Cookies\", 78.1, 6.2, 0.4, 101.8, 0, 9.3, 0.9],\n",
    "                [\"Lowfat Milk\", 121.2, 296.7, 0.1, 500.2, 0, 11.7, 8.1],\n",
    "                [\"Raisin Brn\", 115.1, 12.9, 16.8, 1250.2, 4, 27.9, 4],\n",
    "                [\"Hotdog\", 242.1, 23.5, 2.3, 0, 0, 18, 10.4 ]\n",
    "            ] , columns = [\"Food\",\"Calories\",\"Calcium\",\"Iron\",\"Vit_A\",\"Dietary_Fiber\",\"Carbohydrates\",\"Protein\"])\n",
    "\n",
    "diet_nutrients = pd.DataFrame([\n",
    "                [\"Calories\", 2000, 2500],\n",
    "                [\"Calcium\", 800, 1600],\n",
    "                [\"Iron\", 10, 30],\n",
    "                [\"Vit_A\", 5000, 50000],\n",
    "                [\"Dietary_Fiber\", 25, 100],\n",
    "                [\"Carbohydrates\", 0, 300],\n",
    "                [\"Protein\", 50, 100]\n",
    "            ], columns = [\"name\",\"qmin\",\"qmax\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solve_payload = {\n",
    "    client.deployments.DecisionOptimizationMetaNames.INPUT_DATA: [\n",
    "        {\n",
    "            \"id\":\"diet_food.csv\",\n",
    "            \"values\" : diet_food\n",
    "        },\n",
    "        {\n",
    "            \"id\":\"diet_food_nutrients.csv\",\n",
    "            \"values\" : diet_food_nutrients\n",
    "        },\n",
    "        {\n",
    "            \"id\":\"diet_nutrients.csv\",\n",
    "            \"values\" : diet_nutrients\n",
    "        }\n",
    "    ],\n",
    "    client.deployments.DecisionOptimizationMetaNames.OUTPUT_DATA: [\n",
    "    {\n",
    "        \"id\":\".*\\.csv\"\n",
    "    }\n",
    "    ]\n",
    "}\n",
    "\n",
    "job_details = client.deployments.create_job(deployment_uid, solve_payload)\n",
    "job_uid = client.deployments.get_job_uid(job_details)\n",
    "\n",
    "# print job id if needed\n",
    "# print( job_uid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display job status until it is completed.\n",
    "\n",
    "The first job of a new deployment might take some time as a compute node must be started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "while job_details['entity']['decision_optimization']['status']['state'] not in ['completed', 'failed', 'canceled']:\n",
    "    print(job_details['entity']['decision_optimization']['status']['state'] + '...')\n",
    "    sleep(5)\n",
    "    job_details=client.deployments.get_job_details(job_uid)\n",
    "\n",
    "print( job_details['entity']['decision_optimization']['status']['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='display'></a>\n",
    "### Extract and display solution\n",
    "\n",
    "Display the output solution.\n",
    "\n",
    "Display the KPI Total Calories value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe for the solution\n",
    "solution = pd.DataFrame(job_details['entity']['decision_optimization']['output_data'][0]['values'], \n",
    "                        columns = job_details['entity']['decision_optimization']['output_data'][0]['fields'])\n",
    "solution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( job_details['entity']['decision_optimization']['solve_state']['details']['KPI.Total Calories'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='problem'></a>\n",
    "###  Solve another problem using the same deployment\n",
    "\n",
    "Create a new payload with modified input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the input data\n",
    "diet_nutrients.at[0,'qmin'] = 1500\n",
    "diet_nutrients.at[0,'qmax'] = 2000\n",
    "\n",
    "solve_payload = {\n",
    "    client.deployments.DecisionOptimizationMetaNames.INPUT_DATA: [\n",
    "        {\n",
    "            \"id\":\"diet_food.csv\",\n",
    "            \"values\" : diet_food         \n",
    "        },\n",
    "        {\n",
    "            \"id\":\"diet_food_nutrients.csv\",\n",
    "             \"values\" : diet_food_nutrients            \n",
    "        },\n",
    "        {\n",
    "            \"id\":\"diet_nutrients.csv\",\n",
    "            \"values\" : diet_nutrients\n",
    "        }\n",
    "    ],\n",
    "    client.deployments.DecisionOptimizationMetaNames.OUTPUT_DATA: [\n",
    "    {\n",
    "        \"id\":\".*\\.csv\"\n",
    "    }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = client.deployments.create_job(deployment_uid, solve_payload)\n",
    "job_uid = client.deployments.get_job_uid(job_details)\n",
    "\n",
    "# print job id if needed\n",
    "# print( job_uid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display job status until it is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while job_details['entity']['decision_optimization']['status']['state'] not in ['completed', 'failed', 'canceled']:\n",
    "    print(job_details['entity']['decision_optimization']['status']['state'] + '...')\n",
    "    sleep(5)\n",
    "    job_details=client.deployments.get_job_details(job_uid)\n",
    "\n",
    "print( job_details['entity']['decision_optimization']['status']['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the KPI Total Calories value for this modified data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( job_details['entity']['decision_optimization']['solve_state']['details']['KPI.Total Calories'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.deployments.get_job_details(job_uid)['entity']['decision_optimization']['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following method to delete the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.delete(deployment_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "### Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've successfully completed this notebook! \n",
    "\n",
    "You've learned how to:\n",
    "\n",
    "- work with the Watson Machine Learning client\n",
    "- prepare your model archive and upload your model on Watson Machine Learning\n",
    "- create a deployment\n",
    "- create and monitor a job with inline data for your deployed model\n",
    "- display the solution\n",
    "\n",
    "Check out our online documentation <a href=\"https://www.ibm.com/docs/en/cloud-paks/cp-data/4.6.x\" target=\"_blank\" rel=\"noopener noreferrer\">here</a> for more samples, tutorials and documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright © 2019, 2023. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
